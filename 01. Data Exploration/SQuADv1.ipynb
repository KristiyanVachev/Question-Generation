{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration - SQuAD v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common imports \n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from nltk import tokenize\n",
    "from scipy import stats\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBold(string):\n",
    "    display(Markdown('**' + string + '**'))\n",
    "    \n",
    "#TODO    \n",
    "#def printColor():\n",
    "#     display(Markdown('<span style=\"color:blue\">blue</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "from pathlib import Path\n",
    "\n",
    "def dumpPickle(fileName, content):\n",
    "    pickleFile = open(fileName, 'wb')\n",
    "    cPickle.dump(content, pickleFile, -1)\n",
    "    pickleFile.close()\n",
    "\n",
    "def loadPickle(fileName):    \n",
    "    file = open(fileName, 'rb')\n",
    "    content = cPickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return content\n",
    "    \n",
    "def pickleExists(fileName):\n",
    "    file = Path(fileName)\n",
    "    \n",
    "    if file.is_file():\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aren't really doing the answering of the questions, as is the true intention for the dataset, we'll merge the train and dev datasets into one. The test dataset is probably hidden, since there's a competition for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('../data/squad-v1/train-v1.1.json', orient='column')\n",
    "dev = pd.read_json('../data/squad-v1/dev-v1.1.json', orient='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, dev], ignore_index=True)\n",
    "#merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'University_of_Notre_Dame', 'paragra...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Beyoncé', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Montana', 'paragraphs': [{'context'...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Genocide', 'paragraphs': [{'context...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Antibiotics', 'paragraphs': [{'cont...</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  version\n",
       "0  {'title': 'University_of_Notre_Dame', 'paragra...      1.1\n",
       "1  {'title': 'Beyoncé', 'paragraphs': [{'context'...      1.1\n",
       "2  {'title': 'Montana', 'paragraphs': [{'context'...      1.1\n",
       "3  {'title': 'Genocide', 'paragraphs': [{'context...      1.1\n",
       "4  {'title': 'Antibiotics', 'paragraphs': [{'cont...      1.1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showQuestion(titleId, paragraphId, questionId):\n",
    "\n",
    "    title = df['data'][titleId]['title']\n",
    "    paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "    question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "    answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "    answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "\n",
    "    printBold('Title')\n",
    "    print(title)\n",
    "    printBold('Paragraph')\n",
    "    print(paragraph)\n",
    "    printBold('Question')\n",
    "    print(question)\n",
    "    printBold('Answer')\n",
    "    print(answerStart)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles 490\n",
      "Paragraphs 20963\n",
      "Questions 98169\n"
     ]
    }
   ],
   "source": [
    "titlesCount = len(df['data'])\n",
    "totalParagraphsCount = 0\n",
    "totalQuestionsCount = 0\n",
    "\n",
    "for titleId in range(titlesCount):\n",
    "    paragraphsCount = len(df['data'][titleId]['paragraphs'])\n",
    "    totalParagraphsCount += paragraphsCount\n",
    "    \n",
    "    for paragraphId in range(paragraphsCount):\n",
    "        questionsCount = len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])\n",
    "        \n",
    "        totalQuestionsCount += questionsCount\n",
    "        \n",
    "print('Titles', titlesCount)\n",
    "print('Paragraphs', totalParagraphsCount)\n",
    "print('Questions', totalQuestionsCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      {'title': 'University_of_Notre_Dame', 'paragra...\n",
       "1      {'title': 'Beyoncé', 'paragraphs': [{'context'...\n",
       "2      {'title': 'Montana', 'paragraphs': [{'context'...\n",
       "3      {'title': 'Genocide', 'paragraphs': [{'context...\n",
       "4      {'title': 'Antibiotics', 'paragraphs': [{'cont...\n",
       "                             ...                        \n",
       "485    {'title': 'Islamism', 'paragraphs': [{'context...\n",
       "486    {'title': 'Imperialism', 'paragraphs': [{'cont...\n",
       "487    {'title': 'United_Methodist_Church', 'paragrap...\n",
       "488    {'title': 'French_and_Indian_War', 'paragraphs...\n",
       "489    {'title': 'Force', 'paragraphs': [{'context': ...\n",
       "Name: data, Length: 490, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n",
      "Beyoncé\n",
      "Montana\n",
      "Genocide\n",
      "Antibiotics\n",
      "Frédéric_Chopin\n",
      "Sino-Tibetan_relations_during_the_Ming_dynasty\n",
      "IPod\n",
      "The_Legend_of_Zelda:_Twilight_Princess\n",
      "Spectre_(2015_film)\n",
      "2008_Sichuan_earthquake\n",
      "New_York_City\n",
      "To_Kill_a_Mockingbird\n",
      "Solar_energy\n",
      "Tajikistan\n",
      "Anthropology\n",
      "Portugal\n",
      "Kanye_West\n",
      "Buddhism\n",
      "American_Idol\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "for titleId in range(len(df['data'])):\n",
    "    titles.append(df['data'][titleId]['title'])\n",
    "    \n",
    "for i in range(20):\n",
    "    print(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titles are pretty random. Seems to be a lot of locations like countries and cities but not nearly enough to afford splitting the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our main assumptions is that the sentence that contains the answer could be turned into a question just by removing the answer from it. Let's see how much of that is true for the questions in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSentence(paragraph, answerStart):\n",
    "    \n",
    "    sentences = tokenize.sent_tokenize(paragraph)\n",
    "    sentenceStart = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if (sentenceStart + len(sentence) >= answerStart):\n",
    "            return sentence         \n",
    "        \n",
    "        sentenceStart += len(sentence) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ktmay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "paragraph = df['data'][0]['paragraphs'][0]['context']\n",
    "answerStart = df['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "\n",
    "sentence = extractSentence(paragraph, answerStart)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this can be used for containment score of tect in question\n",
    "def containedInText(text, question):\n",
    "    \n",
    "    questionWords = tokenize.word_tokenize(question.lower())\n",
    "    textWords = tokenize.word_tokenize(text.lower())\n",
    "    wordsContained = 0\n",
    "\n",
    "    for questionWord in questionWords:\n",
    "        for textWord in textWords:\n",
    "            if (questionWord == textWord):\n",
    "                wordsContained += 1\n",
    "                break\n",
    "\n",
    "    print(len(questionWords))\n",
    "    return wordsContained / len(questionWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "question =  df['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "contained = containedInText(sentence, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Sentence**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Contained**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "printBold('Question')\n",
    "print(question)\n",
    "printBold('Sentence')\n",
    "print(sentence)\n",
    "printBold(\"Contained\")\n",
    "print(contained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wouldn't expect a 100% containment simply because the questions will contain **question-like words** like *Why, Who, *Whom*, What*.\n",
    "\n",
    "In this example we also see that the word appear is contained in the original sentence but in **past tense**. We could take care of that if we take the **stems** of the words, but I think it's better to see the least imaginative way for forming questions.\n",
    "\n",
    "We are also calculating some **stopwords - common words like *to, the, in*** which could be encountered at different places of the sentence, but again we want to measure the least-creative questions.\n",
    "\n",
    "In this sentece *(damn, that was a good example)* we also see that the question uses the word *allegedly* which is a **synonym** of *reputedly* in the sentence. That could be nice for question forming, but I think it's more of an overkill.\n",
    "\n",
    "We also see that the question actually encompasses the **words around the answer, rather than the entire sentence**. Which is a definate must-do when we form our questions. \n",
    "\n",
    "Let's see what is the score on all of the questons. I'm also curious to see the score on the entire paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may come in handy in the future. Pretty printing the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printint the percentage completed\n",
    "def printPercentage(currentStep, maxStep):\n",
    "    stepSize = maxStep / 100   #size of each step required to reach 1% progress.\n",
    "    \n",
    "    if (int(currentStep / stepSize) > ((currentStep - 1) / stepSize)):  #checks if the current progress \n",
    "        #is greater than the progress achieved in the previous step\n",
    "        clear_output()\n",
    "        print('{}%'.format(int(currentStep / stepSize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle found. Saved some time.\n"
     ]
    }
   ],
   "source": [
    "questionContainmentDfPickleName = 'pickles/questionContainmentDf.pkl'\n",
    "\n",
    "#If the dataframe is already generated, load it.\n",
    "if (pickleExists(questionContainmentDfPickleName)):\n",
    "    print(\"Pickle found. Saved some time.\")\n",
    "    questionContainmentDf = loadPickle(questionContainmentDfPickleName)\n",
    "else:\n",
    "    sentenceScore = []\n",
    "    paragraphScore = []\n",
    "\n",
    "    #For each title\n",
    "    titlesCount = len(df['data'])\n",
    "    for titleId in range(titlesCount):\n",
    "        printPercentage(titleId, titlesCount)\n",
    "\n",
    "        #For each paragraph\n",
    "        for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "            paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "\n",
    "            #For each question\n",
    "            for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "                question = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['question']\n",
    "                answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "                sentence = extractSentence(paragraph, answerStart) #breaking sentence from para\n",
    "\n",
    "                sentenceScore.append(containedInText(sentence, question))\n",
    "                paragraphScore.append(containedInText(paragraph, question))           \n",
    "                \n",
    "    #Merge dataframes into one                \n",
    "    sentenceScoreDf = pd.DataFrame(sentenceScore, columns=['sentence'])\n",
    "    paragraphScoreDf = pd.DataFrame(paragraphScore, columns=['paragraph'])\n",
    "\n",
    "    questionContainmentDf = pd.concat([sentenceScoreDf, paragraphScoreDf], axis=1)\n",
    "    \n",
    "    #Pickle the result\n",
    "    dumpPickle(questionContainmentDfPickleName, questionContainmentDf)\n",
    "    \n",
    "    print(\"Result not pickled. Generating...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98169.000000</td>\n",
       "      <td>98169.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.463937</td>\n",
       "      <td>0.582157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.190377</td>\n",
       "      <td>0.159055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence     paragraph\n",
       "count  98169.000000  98169.000000\n",
       "mean       0.463937      0.582157\n",
       "std        0.190377      0.159055\n",
       "min        0.000000      0.000000\n",
       "25%        0.333333      0.500000\n",
       "50%        0.461538      0.600000\n",
       "75%        0.600000      0.700000\n",
       "max        1.000000      1.000000"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue that almost half the words contained is a pretty good result. \n",
    "\n",
    "As expected, contained within the entire paragraph is better.\n",
    "\n",
    "I do wonder about those questions that are 100% contained in the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence  paragraph\n",
       "0  0.642857   0.571429\n",
       "1  0.636364   0.636364\n",
       "2  0.533333   0.600000\n",
       "3  0.375000   0.500000\n",
       "4  0.333333   0.416667\n",
       "5  0.272727   0.636364\n",
       "6  0.300000   0.800000\n",
       "7  0.363636   0.727273\n",
       "8  0.000000   0.545455\n",
       "9  0.266667   0.733333"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for specific cases in the questions\n",
    "def getQuestionAt(index):\n",
    "    currentIndex = 0\n",
    "    \n",
    "    for titleId in range(len(df['data'])):\n",
    "        for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "            for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "                if (currentIndex == index):\n",
    "                    return titleId, paragraphId, questionId\n",
    "                currentIndex += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see question #8 which has 0 containment in the answer sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University_of_Notre_Dame\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many student news papers are found at Notre Dame?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "three\n"
     ]
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 1 \n",
    "questionId = 3\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is actually formed from the previous sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0% containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2781</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence  paragraph\n",
       "269        0.0        0.0\n",
       "363        0.0        0.0\n",
       "505        0.0        0.0\n",
       "2781       0.0        0.0\n",
       "3678       0.0        0.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf[questionContainmentDf['paragraph'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did Beyonce start becoming popular?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "in the late 1990s\n"
     ]
    }
   ],
   "source": [
    "titleId = 1\n",
    "paragraphId = 0 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **synonym** case - *instead of rose to fame*, *start becoming popular* is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 18, 6)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(505)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2011, documents obtained by WikiLeaks revealed that Beyoncé was one of many entertainers who performed for the family of Libyan ruler Muammar Gaddafi. Rolling Stone reported that the music industry was urging them to return the money they earned for the concerts; a spokesperson for Beyoncé later confirmed to The Huffington Post that she donated the money to the Clinton Bush Haiti Fund. Later that year she became the first solo female artist to headline the main Pyramid stage at the 2011 Glastonbury Festival in over twenty years, and was named the highest-paid performer in the world per minute.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did this leak happen?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2011\n"
     ]
    }
   ],
   "source": [
    "titleId = 1\n",
    "paragraphId = 18 \n",
    "questionId = 6\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's just a bad question. It could only be asked in combination with the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21911</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39394</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45064</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48874</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53226</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67425</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  paragraph\n",
       "21911       1.0        1.0\n",
       "39394       1.0        1.0\n",
       "45064       1.0        1.0\n",
       "48874       1.0        1.0\n",
       "53226       1.0        1.0\n",
       "67425       1.0        1.0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionContainmentDf[questionContainmentDf['sentence'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258, 23, 0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(53226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utrecht\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utrecht city has an active cultural life, and in the Netherlands is second only to Amsterdam. There are several theatres and theatre companies. The 1941 main city theatre was built by Dudok. Besides theatres there is a large number of cinemas including three arthouse cinemas. Utrecht is host to the international Early Music Festival (Festival Oude Muziek, for music before 1800) and the Netherlands Film Festival. The city has an important classical music hall Vredenburg (1979 by Herman Hertzberger). Its acoustics are considered among the best of the 20th-century original music halls.[citation needed] The original Vredenburg music hall has been redeveloped as part of the larger station area redevelopment plan and in 2014 has gained additional halls that allowed its merger with the rock club Tivoli and the SJU jazzpodium. There are several other venues for music throughout the city. Young musicians are educated in the conservatory, a department of the Utrecht School of the Arts. There is a specialised museum of automatically playing musical instruments.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cultural life in Utrecht is second to \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Utrecht city has an active cultural life, and in the Netherlands is second only to Amsterdam\n"
     ]
    }
   ],
   "source": [
    "titleId = 258\n",
    "paragraphId = 23 \n",
    "questionId = 0\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange question. The question words all appear in the sentence, but not in order. But the answer is the entire sentence, which obviously has needless information inside it. Looking further into it, the question is actually wrong, because it should state second *in Netherlands*. This question should be scrapped..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 25, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getQuestionAt(67425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Title**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Paragraph**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A reversible process is one in which this does not happen.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "dissipation\n"
     ]
    }
   ],
   "source": [
    "titleId = 341\n",
    "paragraphId = 25 \n",
    "questionId = 2\n",
    "\n",
    "showQuestion(titleId, paragraphId, questionId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is, basically, just the question I expect to generate. The answer is removed and the sentence is descriptive enough to fill in the missing word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption that the **question is mostly consisted of words from the sentence the answer is in** seems correct.\n",
    "\n",
    "There are some obvious differences like:\n",
    "- **Question-like words** are added - who, why, when...\n",
    "- **Synonyms** are used instead of the words used in the sentence\n",
    "- Changing the sentence to a question also changes the **tense** of the word.\n",
    "- In long sentences, only a **part of the sentence is used**. Like if the sentence is separated with commas, the comma actually divides two logical statements.\n",
    "\n",
    "I also managed to find some outliers which turned out to be not-so-well asked questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couple of ideas to explore:\n",
    "- Are all the answers phrases from the text\n",
    "- The type of the answers - number, dates, names, locations, similarity to the title\n",
    "- Part of speech - verb, noun\n",
    "- Answer lenght in words\n",
    "- Words around the answer.\n",
    "- Answer location in the sentence - First word, last word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers contained in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Answers in text**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98169\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answers not in text**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "answersInText = 0\n",
    "answersNotInText = 0\n",
    "\n",
    "for titleId in range(len(df['data'])):\n",
    "     for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "        paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "        for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "            answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "            if (answer in paragraph):\n",
    "                answersInText += 1\n",
    "            else:\n",
    "                answersNotInText += 1\n",
    "                \n",
    "printBold('Answers in text')\n",
    "print(answersInText)\n",
    "printBold('Answers not in text')\n",
    "print(answersNotInText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the answers are phrases from the text. Seems like that has been a requirement from the start, since the answers also have an index indicating their start location in the paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "sentences = []\n",
    "\n",
    "for titleId in range(len(df['data'])):\n",
    "    \n",
    "     for paragraphId in range(len(df['data'][titleId]['paragraphs'])):\n",
    "        paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "        \n",
    "        for questionId in range(len(df['data'][titleId]['paragraphs'][paragraphId]['qas'])):\n",
    "            answer = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['text']\n",
    "            answerStart = df['data'][titleId]['paragraphs'][paragraphId]['qas'][questionId]['answers'][0]['answer_start']\n",
    "            #answerStart is the pos of answer. and we fetch the sentences where answer lies\n",
    "            sentence = extractSentence(paragraph, answerStart)\n",
    "            \n",
    "            answers.append(answer)\n",
    "            sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>Immediately in front of the Main Building and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the Main Building</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                            sentence  \n",
       "0  It is a replica of the grotto at Lourdes, Fran...  \n",
       "1  Immediately in front of the Main Building and ...  \n",
       "2  Next to the Main Building is the Basilica of t...  \n",
       "3  Immediately behind the basilica is the Grotto,...  \n",
       "4  Atop the Main Building's gold dome is a golden...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerTextsDf = pd.DataFrame(answers, columns=['answer'])\n",
    "sentenceDf = pd.DataFrame(sentences, columns=['sentence'])\n",
    "\n",
    "answersDf = pd.concat([answerTextsDf, sentenceDf], axis=1)\n",
    "answersDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer word lenght "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = []\n",
    "\n",
    "for i in range(len(answersDf)):\n",
    "    wordCount.append(len(tokenize.word_tokenize(answersDf.iloc[i]['answer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDf = pd.concat([answersDf, pd.DataFrame(wordCount, columns=['wordCount'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                         answer  \\\n",
       "0                   Saint Bernadette Soubirous   \n",
       "1                    a copper statue of Christ   \n",
       "2                            the Main Building   \n",
       "3      a Marian place of prayer and reflection   \n",
       "4           a golden statue of the Virgin Mary   \n",
       "...                                        ...   \n",
       "98164                           kilogram-force   \n",
       "98165                                 kilopond   \n",
       "98166                                     slug   \n",
       "98167                                      kip   \n",
       "98168                                   sthène   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "0      It is a replica of the grotto at Lourdes, Fran...          3  \n",
       "1      Immediately in front of the Main Building and ...          5  \n",
       "2      Next to the Main Building is the Basilica of t...          3  \n",
       "3      Immediately behind the basilica is the Grotto,...          7  \n",
       "4      Atop the Main Building's gold dome is a golden...          7  \n",
       "...                                                  ...        ...  \n",
       "98164  The pound-force has a metric counterpart, less...          1  \n",
       "98165  The pound-force has a metric counterpart, less...          1  \n",
       "98166  The kilogram-force leads to an alternate, but ...          1  \n",
       "98167  Other arcane units of force include the sthène...          1  \n",
       "98168  Other arcane units of force include the sthène...          1  \n",
       "\n",
       "[98169 rows x 3 columns]>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    98169.000000\n",
       "mean         3.355031\n",
       "std          3.731700\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          4.000000\n",
       "max         46.000000\n",
       "Name: wordCount, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['wordCount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCount\n",
       "1     32156\n",
       "2     25228\n",
       "3     14348\n",
       "4      7562\n",
       "5      4659\n",
       "6      3051\n",
       "7      2222\n",
       "8      1676\n",
       "9      1206\n",
       "10      975\n",
       "11      755\n",
       "12      652\n",
       "13      566\n",
       "14      461\n",
       "15      407\n",
       "16      313\n",
       "18      274\n",
       "17      269\n",
       "19      244\n",
       "20      191\n",
       "21      182\n",
       "23      138\n",
       "22      131\n",
       "25      120\n",
       "24      101\n",
       "26       77\n",
       "28       59\n",
       "27       58\n",
       "29       28\n",
       "30       19\n",
       "31       12\n",
       "32       11\n",
       "33        6\n",
       "38        2\n",
       "34        2\n",
       "35        2\n",
       "36        2\n",
       "37        2\n",
       "46        1\n",
       "42        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['wordCount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 1/3 of of the answers are single words. And about 2/3 are up to 3 words. Let's get an overview of the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94041</th>\n",
       "      <td>quickly</td>\n",
       "      <td>As a practice area and specialist domain, phar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16141</th>\n",
       "      <td>1985</td>\n",
       "      <td>By 1985, the USFL had ceased football operatio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>65,000</td>\n",
       "      <td>At 2.7 million in 2012, New York's non-Hispani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70863</th>\n",
       "      <td>Jews</td>\n",
       "      <td>Moving to reduce Italian influence, in October...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19072</th>\n",
       "      <td>148</td>\n",
       "      <td>It has a number of parks and green spaces, the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6351</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>According to Buddhist tradition, the Buddha li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33608</th>\n",
       "      <td>5.5</td>\n",
       "      <td>It is estimated that 5.5 million tonnes of ura...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83840</th>\n",
       "      <td>Hannibal</td>\n",
       "      <td>Extraordinary circumstances called for extraor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>Babylonia</td>\n",
       "      <td>The Roman abacus was used in Babylonia as earl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>arrested</td>\n",
       "      <td>Several protesters who tried to disrupt the re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80591</th>\n",
       "      <td>1930s</td>\n",
       "      <td>The first lighting used on an airport was duri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50348</th>\n",
       "      <td>10</td>\n",
       "      <td>The 2011 domestic Samoan rugby league competit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81972</th>\n",
       "      <td>B-21</td>\n",
       "      <td>The B-21 is projected to replace the B-52 and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50166</th>\n",
       "      <td>Whig</td>\n",
       "      <td>Francis Basset, a backbench Whig MP, wrote to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9063</th>\n",
       "      <td>2006</td>\n",
       "      <td>Business journalist Kimberly Amadeo reports: \"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79467</th>\n",
       "      <td>subtropical</td>\n",
       "      <td>Iran's climate ranges from arid or semiarid, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81182</th>\n",
       "      <td>Samoan</td>\n",
       "      <td>The language has borrowed from the Samoan lang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56902</th>\n",
       "      <td>1.8</td>\n",
       "      <td>The elevation of the area never rises above 40...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>subdivisions</td>\n",
       "      <td>Anthropology has diversified from a few major ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85679</th>\n",
       "      <td>200</td>\n",
       "      <td>Eisenhower was the first non-British person to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             answer                                           sentence  \\\n",
       "94041       quickly  As a practice area and specialist domain, phar...   \n",
       "16141          1985  By 1985, the USFL had ceased football operatio...   \n",
       "4182         65,000  At 2.7 million in 2012, New York's non-Hispani...   \n",
       "70863          Jews  Moving to reduce Italian influence, in October...   \n",
       "19072           148  It has a number of parks and green spaces, the...   \n",
       "6351          Nepal  According to Buddhist tradition, the Buddha li...   \n",
       "33608           5.5  It is estimated that 5.5 million tonnes of ura...   \n",
       "83840      Hannibal  Extraordinary circumstances called for extraor...   \n",
       "23810     Babylonia  The Roman abacus was used in Babylonia as earl...   \n",
       "8244       arrested  Several protesters who tried to disrupt the re...   \n",
       "80591         1930s  The first lighting used on an airport was duri...   \n",
       "50348            10  The 2011 domestic Samoan rugby league competit...   \n",
       "81972          B-21  The B-21 is projected to replace the B-52 and ...   \n",
       "50166          Whig  Francis Basset, a backbench Whig MP, wrote to ...   \n",
       "9063           2006  Business journalist Kimberly Amadeo reports: \"...   \n",
       "79467   subtropical  Iran's climate ranges from arid or semiarid, t...   \n",
       "81182        Samoan  The language has borrowed from the Samoan lang...   \n",
       "56902           1.8  The elevation of the area never rises above 40...   \n",
       "5346   subdivisions  Anthropology has diversified from a few major ...   \n",
       "85679           200  Eisenhower was the first non-British person to...   \n",
       "\n",
       "       wordCount  \n",
       "94041          1  \n",
       "16141          1  \n",
       "4182           1  \n",
       "70863          1  \n",
       "19072          1  \n",
       "6351           1  \n",
       "33608          1  \n",
       "83840          1  \n",
       "23810          1  \n",
       "8244           1  \n",
       "80591          1  \n",
       "50348          1  \n",
       "81972          1  \n",
       "50166          1  \n",
       "9063           1  \n",
       "79467          1  \n",
       "81182          1  \n",
       "56902          1  \n",
       "5346           1  \n",
       "85679          1  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 1].sample(20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot of years and some names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two word answers seem to be dominated by names. There are also a lot of answers where one of the words isn't useful. Some could easily be removed like *a* and *the*. *six years* and *tree times* could also be turned to just 6 and 3. The *13.3%* seems to be just misplaced. Not sure if it's because of the *\".\"* or the *\"%\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62843</th>\n",
       "      <td>25 genes</td>\n",
       "      <td>By the end of 2005, 25 genes had been associat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>Notre Dame</td>\n",
       "      <td>In 2006, Lee was awarded an honorary doctorate...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44145</th>\n",
       "      <td>Charles Pillsbury</td>\n",
       "      <td>There he met fellow student and later Green Pa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68124</th>\n",
       "      <td>Lionel Robbins</td>\n",
       "      <td>With the help of Mises, in the late 1920s Haye...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7152</th>\n",
       "      <td>The Beatles</td>\n",
       "      <td>The single, \"A Moment Like This\", went on to b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37185</th>\n",
       "      <td>Prince Albert</td>\n",
       "      <td>Victoria married her first cousin, Prince Albe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72091</th>\n",
       "      <td>Western Railroad</td>\n",
       "      <td>It was formerly used by the Milwaukee Road fro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>Mockingbird groupies</td>\n",
       "      <td>Local residents call them \"Mockingbird groupie...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89845</th>\n",
       "      <td>two points</td>\n",
       "      <td>Luther's rediscovery of \"Christ and His salvat...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17988</th>\n",
       "      <td>Copeland Award</td>\n",
       "      <td>Kansas also won the 1981–82 Copeland Award.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92556</th>\n",
       "      <td>100–106 °F</td>\n",
       "      <td>The modern bubonic plague has a mortality rate...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15881</th>\n",
       "      <td>Indo-European languages</td>\n",
       "      <td>The Uralic languages do not belong to the Indo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30814</th>\n",
       "      <td>common sense</td>\n",
       "      <td>They begin to differentiate between rules inst...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90452</th>\n",
       "      <td>Dolby Digital</td>\n",
       "      <td>BSkyB's standard definition broadcasts are in ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48587</th>\n",
       "      <td>1 billion</td>\n",
       "      <td>The Carnival industry chain amassed in 2012 al...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>learning investments</td>\n",
       "      <td>Hence the additional costs of the incentives f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59158</th>\n",
       "      <td>built-in microphone</td>\n",
       "      <td>A single device may provide several functions,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7646</th>\n",
       "      <td>Jennifer Hudson</td>\n",
       "      <td>Other alumni have gone on to work in televisio...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59943</th>\n",
       "      <td>elementary ideas</td>\n",
       "      <td>According to Bastian, all human societies shar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82507</th>\n",
       "      <td>Han Chinese</td>\n",
       "      <td>Han Chinese Banners were made up of Han Chines...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        answer  \\\n",
       "62843                 25 genes   \n",
       "4799                Notre Dame   \n",
       "44145        Charles Pillsbury   \n",
       "68124           Lionel Robbins   \n",
       "7152               The Beatles   \n",
       "37185            Prince Albert   \n",
       "72091         Western Railroad   \n",
       "4851      Mockingbird groupies   \n",
       "89845               two points   \n",
       "17988           Copeland Award   \n",
       "92556               100–106 °F   \n",
       "15881  Indo-European languages   \n",
       "30814             common sense   \n",
       "90452            Dolby Digital   \n",
       "48587                1 billion   \n",
       "5075      learning investments   \n",
       "59158      built-in microphone   \n",
       "7646           Jennifer Hudson   \n",
       "59943         elementary ideas   \n",
       "82507              Han Chinese   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "62843  By the end of 2005, 25 genes had been associat...          2  \n",
       "4799   In 2006, Lee was awarded an honorary doctorate...          2  \n",
       "44145  There he met fellow student and later Green Pa...          2  \n",
       "68124  With the help of Mises, in the late 1920s Haye...          2  \n",
       "7152   The single, \"A Moment Like This\", went on to b...          2  \n",
       "37185  Victoria married her first cousin, Prince Albe...          2  \n",
       "72091  It was formerly used by the Milwaukee Road fro...          2  \n",
       "4851   Local residents call them \"Mockingbird groupie...          2  \n",
       "89845  Luther's rediscovery of \"Christ and His salvat...          2  \n",
       "17988        Kansas also won the 1981–82 Copeland Award.          2  \n",
       "92556  The modern bubonic plague has a mortality rate...          2  \n",
       "15881  The Uralic languages do not belong to the Indo...          2  \n",
       "30814  They begin to differentiate between rules inst...          2  \n",
       "90452  BSkyB's standard definition broadcasts are in ...          2  \n",
       "48587  The Carnival industry chain amassed in 2012 al...          2  \n",
       "5075   Hence the additional costs of the incentives f...          2  \n",
       "59158  A single device may provide several functions,...          2  \n",
       "7646   Other alumni have gone on to work in televisio...          2  \n",
       "59943  According to Bastian, all human societies shar...          2  \n",
       "82507  Han Chinese Banners were made up of Han Chines...          2  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 2].sample(n=20, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two word answers seem to be dominated by names. There are also a lot of answers where one of the words isn't useful. Some could easily be removed like *a*, *in* and *the*. *six years* could be turned to just 6. The *23.02%* seems to be just misplaced. Not sure if it's because of the *\".\"* or the *\"%\"*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76445</th>\n",
       "      <td>the CAP theorem</td>\n",
       "      <td>In recent years there was a high demand for ma...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28489</th>\n",
       "      <td>Futbol Club Barcelona</td>\n",
       "      <td>With the end of Franco's dictatorship in 1974,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85710</th>\n",
       "      <td>21st Army Group</td>\n",
       "      <td>Field Marshal Montgomery insisted priority be ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96528</th>\n",
       "      <td>in C4 plants</td>\n",
       "      <td>Cyclic photophosphorylation is common in C4 pl...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61068</th>\n",
       "      <td>970 and 1190</td>\n",
       "      <td>The Chalukya dynasty ruled parts of southern a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33328</th>\n",
       "      <td>General Auguste-Alexandre Ducrot</td>\n",
       "      <td>What made a bad situation much worse was the c...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64822</th>\n",
       "      <td>inside the egg</td>\n",
       "      <td>The fertilization and development takes place ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24545</th>\n",
       "      <td>the South site</td>\n",
       "      <td>Eventually, owing to space constraints and the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54848</th>\n",
       "      <td>Stop TB Partnership</td>\n",
       "      <td>The World Health Organization declared TB a \"g...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84203</th>\n",
       "      <td>the Roku player</td>\n",
       "      <td>Google made YouTube available on the Roku play...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33048</th>\n",
       "      <td>Korea and Vietnam</td>\n",
       "      <td>During the Cold War, American troops and their...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78069</th>\n",
       "      <td>successes against Catiline</td>\n",
       "      <td>The Senate, elated by its successes against Ca...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85670</th>\n",
       "      <td>George C. Marshall</td>\n",
       "      <td>Next, he was appointed Assistant Chief of Staf...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10430</th>\n",
       "      <td>Pius V.</td>\n",
       "      <td>The use of the title was reserved for the card...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49201</th>\n",
       "      <td>Frederick the Wise</td>\n",
       "      <td>When he refused, he was placed under the ban o...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>Reporters Without Borders</td>\n",
       "      <td>Reporters Without Borders organised several sy...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82432</th>\n",
       "      <td>mythical chullumpi bird</td>\n",
       "      <td>The mythical chullumpi bird is said to mark th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50021</th>\n",
       "      <td>health and infrastructure</td>\n",
       "      <td>There is a strong belief on the island that so...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53592</th>\n",
       "      <td>the Manchester Guardian</td>\n",
       "      <td>The treaty was published in the United States ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85170</th>\n",
       "      <td>James A. Garfield</td>\n",
       "      <td>Further, when Republicans were in the minority...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 answer  \\\n",
       "76445                   the CAP theorem   \n",
       "28489             Futbol Club Barcelona   \n",
       "85710                   21st Army Group   \n",
       "96528                      in C4 plants   \n",
       "61068                      970 and 1190   \n",
       "33328  General Auguste-Alexandre Ducrot   \n",
       "64822                    inside the egg   \n",
       "24545                    the South site   \n",
       "54848               Stop TB Partnership   \n",
       "84203                   the Roku player   \n",
       "33048                 Korea and Vietnam   \n",
       "78069        successes against Catiline   \n",
       "85670                George C. Marshall   \n",
       "10430                           Pius V.   \n",
       "49201                Frederick the Wise   \n",
       "8290          Reporters Without Borders   \n",
       "82432           mythical chullumpi bird   \n",
       "50021         health and infrastructure   \n",
       "53592           the Manchester Guardian   \n",
       "85170                 James A. Garfield   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "76445  In recent years there was a high demand for ma...          3  \n",
       "28489  With the end of Franco's dictatorship in 1974,...          3  \n",
       "85710  Field Marshal Montgomery insisted priority be ...          3  \n",
       "96528  Cyclic photophosphorylation is common in C4 pl...          3  \n",
       "61068  The Chalukya dynasty ruled parts of southern a...          3  \n",
       "33328  What made a bad situation much worse was the c...          3  \n",
       "64822  The fertilization and development takes place ...          3  \n",
       "24545  Eventually, owing to space constraints and the...          3  \n",
       "54848  The World Health Organization declared TB a \"g...          3  \n",
       "84203  Google made YouTube available on the Roku play...          3  \n",
       "33048  During the Cold War, American troops and their...          3  \n",
       "78069  The Senate, elated by its successes against Ca...          3  \n",
       "85670  Next, he was appointed Assistant Chief of Staf...          3  \n",
       "10430  The use of the title was reserved for the card...          3  \n",
       "49201  When he refused, he was placed under the ban o...          3  \n",
       "8290   Reporters Without Borders organised several sy...          3  \n",
       "82432  The mythical chullumpi bird is said to mark th...          3  \n",
       "50021  There is a strong belief on the island that so...          3  \n",
       "53592  The treaty was published in the United States ...          3  \n",
       "85170  Further, when Republicans were in the minority...          3  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 3].sample(n=20, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again names, more institution names as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23833</th>\n",
       "      <td>system of pulleys and wires</td>\n",
       "      <td>It used a system of pulleys and wires to autom...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46951</th>\n",
       "      <td>Inner London and Outer London</td>\n",
       "      <td>Greater London is split for some purposes into...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97610</th>\n",
       "      <td>within the Church of England</td>\n",
       "      <td>The movement which would become The United Met...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94650</th>\n",
       "      <td>Annual Status of Education Report</td>\n",
       "      <td>The Annual Status of Education Report (ASER), ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23462</th>\n",
       "      <td>less than one per cent</td>\n",
       "      <td>Throughout the period monks remained a very sm...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32401</th>\n",
       "      <td>a system of concentric layers</td>\n",
       "      <td>Air defence in naval tactics, especially withi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20665</th>\n",
       "      <td>the structure of the Alps</td>\n",
       "      <td>In simple terms the structure of the Alps cons...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74712</th>\n",
       "      <td>quadrivium and scholastic logic.</td>\n",
       "      <td>The people were associated with the studia hum...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22411</th>\n",
       "      <td>poor management and financial control</td>\n",
       "      <td>The Ministry of Defence has been criticised in...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76929</th>\n",
       "      <td>between 1 and 1.5 million</td>\n",
       "      <td>The total number of people killed has been mos...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75837</th>\n",
       "      <td>the Nizams and the British</td>\n",
       "      <td>:18 Many elite clubs formed by the Nizams and ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18617</th>\n",
       "      <td>political sentiments of the time</td>\n",
       "      <td>There was also a rise, especially toward the e...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58067</th>\n",
       "      <td>Renaissance polyphony and Baroque concertato</td>\n",
       "      <td>The term \"a cappella\" was originally intended ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48614</th>\n",
       "      <td>the social and political situation</td>\n",
       "      <td>Traditionally formed by men and now starting t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27628</th>\n",
       "      <td>a decimal system of values</td>\n",
       "      <td>Unlike the Spanish milled dollar the U.S. doll...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84787</th>\n",
       "      <td>doctrine of the two kingdoms</td>\n",
       "      <td>Martin Luther separated the religious and the ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50806</th>\n",
       "      <td>high speed and light weight</td>\n",
       "      <td>This makes them useful for appliances such as ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68191</th>\n",
       "      <td>Profits, Interest and Investment</td>\n",
       "      <td>Hayek continued his research on monetary and c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40959</th>\n",
       "      <td>several hundred to 2,000 hours</td>\n",
       "      <td>The trade-off is typically set to provide a li...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64151</th>\n",
       "      <td>North Dakota's Bakken Formation</td>\n",
       "      <td>Prudhoe Bay on Alaska's North Slope is still t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answer  \\\n",
       "23833                   system of pulleys and wires   \n",
       "46951                 Inner London and Outer London   \n",
       "97610                  within the Church of England   \n",
       "94650             Annual Status of Education Report   \n",
       "23462                        less than one per cent   \n",
       "32401                 a system of concentric layers   \n",
       "20665                     the structure of the Alps   \n",
       "74712              quadrivium and scholastic logic.   \n",
       "22411         poor management and financial control   \n",
       "76929                     between 1 and 1.5 million   \n",
       "75837                    the Nizams and the British   \n",
       "18617              political sentiments of the time   \n",
       "58067  Renaissance polyphony and Baroque concertato   \n",
       "48614            the social and political situation   \n",
       "27628                    a decimal system of values   \n",
       "84787                  doctrine of the two kingdoms   \n",
       "50806                   high speed and light weight   \n",
       "68191              Profits, Interest and Investment   \n",
       "40959                several hundred to 2,000 hours   \n",
       "64151               North Dakota's Bakken Formation   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "23833  It used a system of pulleys and wires to autom...          5  \n",
       "46951  Greater London is split for some purposes into...          5  \n",
       "97610  The movement which would become The United Met...          5  \n",
       "94650  The Annual Status of Education Report (ASER), ...          5  \n",
       "23462  Throughout the period monks remained a very sm...          5  \n",
       "32401  Air defence in naval tactics, especially withi...          5  \n",
       "20665  In simple terms the structure of the Alps cons...          5  \n",
       "74712  The people were associated with the studia hum...          5  \n",
       "22411  The Ministry of Defence has been criticised in...          5  \n",
       "76929  The total number of people killed has been mos...          5  \n",
       "75837  :18 Many elite clubs formed by the Nizams and ...          5  \n",
       "18617  There was also a rise, especially toward the e...          5  \n",
       "58067  The term \"a cappella\" was originally intended ...          5  \n",
       "48614  Traditionally formed by men and now starting t...          5  \n",
       "27628  Unlike the Spanish milled dollar the U.S. doll...          5  \n",
       "84787  Martin Luther separated the religious and the ...          5  \n",
       "50806  This makes them useful for appliances such as ...          5  \n",
       "68191  Hayek continued his research on monetary and c...          5  \n",
       "40959  The trade-off is typically set to provide a li...          5  \n",
       "64151  Prudhoe Bay on Alaska's North Slope is still t...          5  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 5].sample(n=20, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the words increase it seems harder to create deceptive incorrect answers. A viable option for some would to be mix the individual words like:\n",
    "\n",
    "*end of World War I\" -> start of World War 1, end of World War II, start of World War II, end of Balkans Wars*....\n",
    "\n",
    "*large tumour on her liver -> large tumor on her brain, large tumor on her lungs, large (some other medical term) on her liver*\n",
    "\n",
    "Though this would become more difficult because if use 2 generated words, they must also fit with each other as well as the original words.\n",
    "\n",
    "Some of the anwers look like logical phrases. For their generation I would argue that a text-summarization aproach would work. And with longer answers we could employ a **True/False** questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14715</th>\n",
       "      <td>to saturate broken (\"dangling\") bonds of amorp...</td>\n",
       "      <td>Hydrogen is employed to saturate broken (\"dang...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70704</th>\n",
       "      <td>Bullied for being a Bedouin, he was proud of h...</td>\n",
       "      <td>Bullied for being a Bedouin, he was proud of h...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39422</th>\n",
       "      <td>the Bill &amp; Melinda Gates Foundation Trust, whi...</td>\n",
       "      <td>In October 2006, the Bill &amp; Melinda Gates Foun...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21062</th>\n",
       "      <td>There are 64 possible codons (four possible nu...</td>\n",
       "      <td>:6 Additionally, a \"start codon\", and three \"s...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>Francesinha (Frenchie) from Porto, and bifanas...</td>\n",
       "      <td>Typical fast food dishes include the Francesin...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34718</th>\n",
       "      <td>into four summaries that look specifically at ...</td>\n",
       "      <td>However, results can be further simplified int...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26946</th>\n",
       "      <td>elected members and special office bearers suc...</td>\n",
       "      <td>The legislature consists of elected members an...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96100</th>\n",
       "      <td>support from China for a planned $2.5 billion ...</td>\n",
       "      <td>Kenyatta was \"[a]ccompanied by 60 Kenyan busin...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77789</th>\n",
       "      <td>On 26 December 1999, Chelsea became the first ...</td>\n",
       "      <td>On 26 December 1999, Chelsea became the first ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97887</th>\n",
       "      <td>format of the congress and many specifics of t...</td>\n",
       "      <td>Nevertheless, the format of the congress and m...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  answer  \\\n",
       "14715  to saturate broken (\"dangling\") bonds of amorp...   \n",
       "70704  Bullied for being a Bedouin, he was proud of h...   \n",
       "39422  the Bill & Melinda Gates Foundation Trust, whi...   \n",
       "21062  There are 64 possible codons (four possible nu...   \n",
       "5882   Francesinha (Frenchie) from Porto, and bifanas...   \n",
       "34718  into four summaries that look specifically at ...   \n",
       "26946  elected members and special office bearers suc...   \n",
       "96100  support from China for a planned $2.5 billion ...   \n",
       "77789  On 26 December 1999, Chelsea became the first ...   \n",
       "97887  format of the congress and many specifics of t...   \n",
       "\n",
       "                                                sentence  wordCount  \n",
       "14715  Hydrogen is employed to saturate broken (\"dang...         20  \n",
       "70704  Bullied for being a Bedouin, he was proud of h...         20  \n",
       "39422  In October 2006, the Bill & Melinda Gates Foun...         20  \n",
       "21062  :6 Additionally, a \"start codon\", and three \"s...         20  \n",
       "5882   Typical fast food dishes include the Francesin...         20  \n",
       "34718  However, results can be further simplified int...         20  \n",
       "26946  The legislature consists of elected members an...         20  \n",
       "96100  Kenyatta was \"[a]ccompanied by 60 Kenyan busin...         20  \n",
       "77789  On 26 December 1999, Chelsea became the first ...         20  \n",
       "97887  Nevertheless, the format of the congress and m...         20  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 20].sample(n=10, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On 26 December 1999, Chelsea became the first Premier League side to field an entirely foreign starting line-up,'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 20].sample(n=20, random_state=5).iloc[8]['answer']\n",
    "\n",
    "#from 8th row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue that from this sentence could be created several questions with single word answers, like:\n",
    "- In what year? - *1999*\n",
    "- Which team? - *Chelsea*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our longest answer with 46 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that the sudden shift of a huge quantity of water into the region could have relaxed the tension between the two sides of the fault, allowing them to move apart, and could have increased the direct pressure on it, causing a violent rupture'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 46].iloc[0]['answer']  #from 1st row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sudden shift of a huge quantity of water* seems like a good answer to the question *What could have relaxed the tension between the two sides?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hillary Clinton (2008), Howard Dean (2004), Gary Hart (1984 and 1988), Paul Tsongas (1992), Pat Robertson (1988) and Jerry Brown (1976, 1980, 1992).'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['wordCount'] == 42].iloc[0]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second longest answer seems to be a sequence of correct answer, to something like *Who has been a presitend candidate*. This could be great for queastion with multiple correct answers as well as multiple incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy** turned out be a pretty great tool which could provide me with *NER (Named entity recognition), part of speech detection, word embeddings similarity* and some more functions which may or may be useful in my case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'NORP'), ('Google', 'ORG'), ('$5.1 billion', 'MONEY'), ('Wednesday', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\n",
    "print([(X.text, X.label_) for X in doc.ents])   #entity labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerForWord(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    entitiesFound = len(doc.ents)\n",
    "    \n",
    "    if (entitiesFound > 0):\n",
    "        #TODO - Could potentially find multiple entities in the text. We're returning only the first one.\n",
    "        return doc.ents[0].label_\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPE'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NerForWord('Portugal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful function for deciphering the tags. They really go deep into the grammatical types, most of which I haven't even heard  of until now. I suspect I'll have to group them up or not use some of the information at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"dobj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the *spacy* tagging works on tokens (not necessarily single words, could be multiple words, e.g. names) it'll significatly ease my work if (for now) I work only with the answers which contain only 1 token. \n",
    "\n",
    "By my judgment, most of the multiple-token answers contain a single important token and a few words describing it. Or are multiple correct tokens separted by 'and' or ','. I could try to extract the important tokens, but I don't think it's worth it at this point.\n",
    "\n",
    "There are some great questions containing multi-token answers, but I think it's better If I limit myself to only single-token answers. That way I can work easier with word embedings and detect the tokens appropriate to be answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSingleToken(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #The entire text is a single named entity \n",
    "    entitiesFound = len(doc.ents)\n",
    "    if(entitiesFound == 1 and doc.ents[0].text == text):   #.text, .label_ \n",
    "        return True\n",
    "    \n",
    "    #The text is not an named entity, but is a single token\n",
    "    tokensFound = len(doc)\n",
    "    if (tokensFound == 1):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSingleToken('George R. R. Martin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many of our answers we're gonna cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "singleTokenCount = 0\n",
    "\n",
    "sampleSize =  int(len(answersDf) / 10)\n",
    "for i in range(sampleSize):\n",
    "        \n",
    "    printPercentage(i, sampleSize)\n",
    "    \n",
    "    if (isSingleToken(answersDf.iloc[i]['answer'])):\n",
    "        singleTokenCount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5755908720456397"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleTokenCount / sampleSize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 10% of the data about 60% is retained. I expected worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some of the more interesting spacy tags - NER, POS, DEP, TAG, SHAPE...\n",
    "\n",
    "Better to provide the full text to spacy, because the token's tags are influenced by their relationship with the other words in the text. But we'll do that we do the feature engineering and also tag the non-answer words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James James PROPN NNP compound Xxxxx True False 1 PERSON\n",
      "R. R. PROPN NNP compound X. False False 1 PERSON\n",
      "Scott Scott PROPN NNP ROOT Xxxxx True False 1 PERSON\n",
      "Xxxxx X. Xxxxx\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('James R. Scott')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop, len(doc.ents), doc.ents[0].label_)\n",
    "    \n",
    "shape = doc[0].shape_\n",
    "for wordIndex in range(1, len(doc)):   #shape or visual patter of taken Xxxxx\n",
    "    shape += (' ' + doc[wordIndex].shape_)\n",
    "        \n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numerals that do not fall under another type'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('CARDINAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersDf['isSingleToken'] = False\n",
    "answersDf['NER'] = ''   # entity recognition\n",
    "answersDf['POS'] = ''  #parts of speeech\n",
    "answersDf['TAG'] = ''   #tags\n",
    "answersDf['DEP'] = ''   #dependency\n",
    "answersDf['shape'] = ''   # shape attribute\n",
    "answersDf['isAlpha'] = False    #alphabet\n",
    "answersDf['isStop'] = False   #stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>It is a replica of the grotto at Lourdes, Fran...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>Immediately in front of the Main Building and ...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the Main Building</td>\n",
       "      <td>Next to the Main Building is the Basilica of t...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>Immediately behind the basilica is the Grotto,...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>Atop the Main Building's gold dome is a golden...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    answer  \\\n",
       "0               Saint Bernadette Soubirous   \n",
       "1                a copper statue of Christ   \n",
       "2                        the Main Building   \n",
       "3  a Marian place of prayer and reflection   \n",
       "4       a golden statue of the Virgin Mary   \n",
       "\n",
       "                                            sentence  wordCount  \\\n",
       "0  It is a replica of the grotto at Lourdes, Fran...          3   \n",
       "1  Immediately in front of the Main Building and ...          5   \n",
       "2  Next to the Main Building is the Basilica of t...          3   \n",
       "3  Immediately behind the basilica is the Grotto,...          7   \n",
       "4  Atop the Main Building's gold dome is a golden...          7   \n",
       "\n",
       "   isSingleToken NER POS TAG DEP shape  isAlpha  isStop  \n",
       "0          False                          False   False  \n",
       "1          False                          False   False  \n",
       "2          False                          False   False  \n",
       "3          False                          False   False  \n",
       "4          False                          False   False  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populating the single-token answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "singleTokenCount = 0\n",
    "\n",
    "sampleSize = int(len(answersDf) / 10)\n",
    "\n",
    "for i in range(sampleSize):\n",
    "        \n",
    "    printPercentage(i, sampleSize)\n",
    "    \n",
    "    answer = answersDf.iloc[i]['answer']\n",
    "    if (isSingleToken(answer)):\n",
    "        answersDf.at[i, 'isSingleToken'] = True\n",
    "        \n",
    "        answersDf.at[i, 'NER'] = NerForWord(answer)\n",
    "        \n",
    "        #At this point I've called spacy's nlp method 3 times for the same words...\n",
    "        doc = nlp(answer)\n",
    "        \n",
    "        answersDf.at[i, 'POS'] = doc[0].pos_   #.at[ ] pandas method to access/modify single entity\n",
    "        answersDf.at[i, 'TAG'] = doc[0].tag_\n",
    "        answersDf.at[i, 'DEP'] = doc[0].dep_\n",
    "        answersDf.at[i, 'isAlpha'] = doc[0].is_alpha\n",
    "        answersDf.at[i, 'isStop'] = doc[0].is_stop\n",
    "        \n",
    "        shape = doc[0].shape_\n",
    "        for wordIndex in range(1, len(doc)):\n",
    "            shape += (' ' + doc[wordIndex].shape_)\n",
    "            \n",
    "        answersDf.at[i, 'shape'] = shape\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isStop\n",
       "False    97675\n",
       "True       494\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['isStop'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely not bother with stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NER\n",
       "               93737\n",
       "PERSON          1044\n",
       "CARDINAL         965\n",
       "DATE             958\n",
       "ORG              562\n",
       "GPE              353\n",
       "PERCENT          151\n",
       "MONEY             99\n",
       "NORP              98\n",
       "LOC               53\n",
       "FAC               37\n",
       "ORDINAL           34\n",
       "QUANTITY          32\n",
       "TIME              14\n",
       "WORK_OF_ART       11\n",
       "EVENT              9\n",
       "LANGUAGE           7\n",
       "LAW                3\n",
       "PRODUCT            2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['NER'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that I've done the NER on only 10% of the dataset. So, 9350 out of 93503. Seems about 40% of the word have a NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>On the occasion of the composer's bicentenary,...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>Xxx Xxx Xxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5060</th>\n",
       "      <td>The International Energy Agency</td>\n",
       "      <td>The International Energy Agency has said that ...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td>det</td>\n",
       "      <td>Xxx Xxxxx Xxxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>KK</td>\n",
       "      <td>The present standard musicological reference f...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>XX</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>Queens Borough Public Library</td>\n",
       "      <td>Queens is served by the Queens Borough Public ...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxxxx Xxxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>Tzu Chi Foundation</td>\n",
       "      <td>Beijing accepted the aid of the Tzu Chi Founda...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxx Xxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>Federal Hall</td>\n",
       "      <td>In 1789, the first President of the United Sta...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>eolomelodicon</td>\n",
       "      <td>He was engaged by the inventors of a mechanica...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8597</th>\n",
       "      <td>CNN</td>\n",
       "      <td>On April 17, Xinhua condemned what it called \"...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>XXX</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>Krugman</td>\n",
       "      <td>Krugman's contention (that the growth of a com...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>Apple</td>\n",
       "      <td>The iPod is a line of portable media players a...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               answer  \\\n",
       "1937               The New York Times   \n",
       "5060  The International Energy Agency   \n",
       "2080                               KK   \n",
       "4342    Queens Borough Public Library   \n",
       "3635               Tzu Chi Foundation   \n",
       "3928                     Federal Hall   \n",
       "1618                    eolomelodicon   \n",
       "8597                              CNN   \n",
       "9061                          Krugman   \n",
       "2477                            Apple   \n",
       "\n",
       "                                               sentence  wordCount  \\\n",
       "1937  On the occasion of the composer's bicentenary,...          4   \n",
       "5060  The International Energy Agency has said that ...          4   \n",
       "2080  The present standard musicological reference f...          1   \n",
       "4342  Queens is served by the Queens Borough Public ...          4   \n",
       "3635  Beijing accepted the aid of the Tzu Chi Founda...          3   \n",
       "3928  In 1789, the first President of the United Sta...          2   \n",
       "1618  He was engaged by the inventors of a mechanica...          1   \n",
       "8597  On April 17, Xinhua condemned what it called \"...          1   \n",
       "9061  Krugman's contention (that the growth of a com...          1   \n",
       "2477  The iPod is a line of portable media players a...          1   \n",
       "\n",
       "      isSingleToken  NER    POS  TAG       DEP                    shape  \\\n",
       "1937           True  ORG    DET   DT       det       Xxx Xxx Xxxx Xxxxx   \n",
       "5060           True  ORG    DET   DT       det    Xxx Xxxxx Xxxxx Xxxxx   \n",
       "2080           True  ORG  PROPN  NNP      ROOT                       XX   \n",
       "4342           True  ORG  PROPN  NNP  compound  Xxxxx Xxxxx Xxxxx Xxxxx   \n",
       "3635           True  ORG  PROPN  NNP  compound            Xxx Xxx Xxxxx   \n",
       "3928           True  ORG  PROPN  NNP  compound               Xxxxx Xxxx   \n",
       "1618           True  ORG  PROPN  NNP      ROOT                     xxxx   \n",
       "8597           True  ORG  PROPN  NNP      ROOT                      XXX   \n",
       "9061           True  ORG  PROPN  NNP      ROOT                    Xxxxx   \n",
       "2477           True  ORG   NOUN   NN      ROOT                    Xxxxx   \n",
       "\n",
       "      isAlpha  isStop  \n",
       "1937     True    True  \n",
       "5060     True    True  \n",
       "2080     True   False  \n",
       "4342     True   False  \n",
       "3635     True   False  \n",
       "3928     True   False  \n",
       "1618     True   False  \n",
       "8597     True   False  \n",
       "9061     True   False  \n",
       "2477     True   False  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['NER'] == 'ORG'].sample(n=10, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isAlpha\n",
       "False    94146\n",
       "True      4023\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['isAlpha'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS\n",
       "         92519\n",
       "PROPN     2267\n",
       "NUM       1715\n",
       "NOUN       874\n",
       "ADJ        294\n",
       "VERB       167\n",
       "DET        134\n",
       "SYM         72\n",
       "ADV         54\n",
       "ADP         26\n",
       "X           22\n",
       "PRON        10\n",
       "PUNCT        6\n",
       "INTJ         5\n",
       "AUX          3\n",
       "PART         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf['POS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers are dominated by nouns. Difference between a noun and a proper noun (PROPN) is that proper nouns are names of specific people, places, ideas... while common nouns are just non-specific (cat, woman, bottle...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>The Hudson River separates the city from the U...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>GPE</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Frank Eck Stadium</td>\n",
       "      <td>Also, there are many outdoor fields, as the Fr...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>Arthur Hutchings</td>\n",
       "      <td>While his illness and his love-affairs conform...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9178</th>\n",
       "      <td>Merrill Lynch</td>\n",
       "      <td>The volume \"Credit Correlation: Life After Cop...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>Zhang Juzheng</td>\n",
       "      <td>Before he left, he sent a letter and gifts to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>Xxxxx Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 answer                                           sentence  \\\n",
       "4022         New Jersey  The Hudson River separates the city from the U...   \n",
       "152   Frank Eck Stadium  Also, there are many outdoor fields, as the Fr...   \n",
       "2167   Arthur Hutchings  While his illness and his love-affairs conform...   \n",
       "9178      Merrill Lynch  The volume \"Credit Correlation: Life After Cop...   \n",
       "2431      Zhang Juzheng  Before he left, he sent a letter and gifts to ...   \n",
       "\n",
       "      wordCount  isSingleToken     NER    POS  TAG       DEP            shape  \\\n",
       "4022          2           True     GPE  PROPN  NNP  compound        Xxx Xxxxx   \n",
       "152           3           True  PERSON  PROPN  NNP  compound  Xxxxx Xxx Xxxxx   \n",
       "2167          2           True  PERSON  PROPN  NNP  compound      Xxxxx Xxxxx   \n",
       "9178          2           True     ORG  PROPN  NNP  compound      Xxxxx Xxxxx   \n",
       "2431          2           True  PERSON  PROPN  NNP  compound      Xxxxx Xxxxx   \n",
       "\n",
       "      isAlpha  isStop  \n",
       "4022     True   False  \n",
       "152      True   False  \n",
       "2167     True   False  \n",
       "9178     True   False  \n",
       "2431     True   False  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['POS'] == 'PROPN'].sample(n=5, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>jazz</td>\n",
       "      <td>The city was a center of jazz in the 1940s, ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>meditation</td>\n",
       "      <td>While there is no convincing evidence for medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7765</th>\n",
       "      <td>dukkōn</td>\n",
       "      <td>The term may possibly derive from Proto-German...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>intelligence</td>\n",
       "      <td>Dog intelligence is the ability of the dog to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6772</th>\n",
       "      <td>shramanas</td>\n",
       "      <td>[note 16] These groups, whose members were kno...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            answer                                           sentence  \\\n",
       "4394          jazz  The city was a center of jazz in the 1940s, ab...   \n",
       "6695    meditation  While there is no convincing evidence for medi...   \n",
       "7765        dukkōn  The term may possibly derive from Proto-German...   \n",
       "7889  intelligence  Dog intelligence is the ability of the dog to ...   \n",
       "6772     shramanas  [note 16] These groups, whose members were kno...   \n",
       "\n",
       "      wordCount  isSingleToken NER   POS  TAG   DEP shape  isAlpha  isStop  \n",
       "4394          1           True      NOUN   NN  ROOT  xxxx     True   False  \n",
       "6695          1           True      NOUN   NN  ROOT  xxxx     True   False  \n",
       "7765          1           True      NOUN   NN  ROOT  xxxx     True   False  \n",
       "7889          1           True      NOUN   NN  ROOT  xxxx     True   False  \n",
       "6772          1           True      NOUN  NNS  ROOT  xxxx     True   False  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['POS'] == 'NOUN'].sample(n=5, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second most prominent category is NUM. It's pretty much years and other numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6880</th>\n",
       "      <td>487 million</td>\n",
       "      <td>According to Johnson and Grim (2013), Buddhism...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>compound</td>\n",
       "      <td>ddd xxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>1931</td>\n",
       "      <td>The Art Deco style of the Chrysler Building (1...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9605</th>\n",
       "      <td>1975</td>\n",
       "      <td>By 1975 the majority of local authorities in E...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3264</th>\n",
       "      <td>48</td>\n",
       "      <td>On Metacritic, the film has a rating of 60 out...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>1565</td>\n",
       "      <td>In 1565, the powerful Rinbung princes were ove...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2007</td>\n",
       "      <td>In 2007, Apple modified the iPod interface aga...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>26 February 1832</td>\n",
       "      <td>On 26 February 1832 Chopin gave a debut Paris ...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>dd Xxxxx dddd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>4.40%</td>\n",
       "      <td>The United States Census Bureau estimates that...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>PERCENT</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>nummod</td>\n",
       "      <td>d.dd %</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8389</th>\n",
       "      <td>70</td>\n",
       "      <td>\\n India: Due to concerns about pro-Tibet prot...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>dd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7123</th>\n",
       "      <td>seven</td>\n",
       "      <td>Starting with season seven, contestants may pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NUM</td>\n",
       "      <td>CD</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                answer                                           sentence  \\\n",
       "6880       487 million  According to Johnson and Grim (2013), Buddhism...   \n",
       "4054              1931  The Art Deco style of the Chrysler Building (1...   \n",
       "9605              1975  By 1975 the majority of local authorities in E...   \n",
       "3264                48  On Metacritic, the film has a rating of 60 out...   \n",
       "2448              1565  In 1565, the powerful Rinbung princes were ove...   \n",
       "2514              2007  In 2007, Apple modified the iPod interface aga...   \n",
       "1742  26 February 1832  On 26 February 1832 Chopin gave a debut Paris ...   \n",
       "1137             4.40%  The United States Census Bureau estimates that...   \n",
       "8389                70  \\n India: Due to concerns about pro-Tibet prot...   \n",
       "7123             seven  Starting with season seven, contestants may pe...   \n",
       "\n",
       "      wordCount  isSingleToken       NER  POS TAG       DEP          shape  \\\n",
       "6880          2           True  CARDINAL  NUM  CD  compound       ddd xxxx   \n",
       "4054          1           True      DATE  NUM  CD      ROOT           dddd   \n",
       "9605          1           True      DATE  NUM  CD      ROOT           dddd   \n",
       "3264          1           True  CARDINAL  NUM  CD      ROOT             dd   \n",
       "2448          1           True      DATE  NUM  CD      ROOT           dddd   \n",
       "2514          1           True      DATE  NUM  CD      ROOT           dddd   \n",
       "1742          3           True      DATE  NUM  CD    nummod  dd Xxxxx dddd   \n",
       "1137          2           True   PERCENT  NUM  CD    nummod         d.dd %   \n",
       "8389          1           True  CARDINAL  NUM  CD      ROOT             dd   \n",
       "7123          1           True  CARDINAL  NUM  CD      ROOT           xxxx   \n",
       "\n",
       "      isAlpha  isStop  \n",
       "6880    False   False  \n",
       "4054    False   False  \n",
       "9605    False   False  \n",
       "3264    False   False  \n",
       "2448    False   False  \n",
       "2514    False   False  \n",
       "1742    False   False  \n",
       "1137    False   False  \n",
       "8389    False   False  \n",
       "7123     True   False  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[answersDf['POS'] == 'NUM'].sample(n=10, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adjectives and Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't really expect much of those, but they seem like adequate answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>Republican</td>\n",
       "      <td>New York City has not been carried by a Republ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NORP</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>Galician-Portuguese</td>\n",
       "      <td>Portuguese is a Romance language that originat...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NORP</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>amod</td>\n",
       "      <td>Xxxxx - Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>monastic</td>\n",
       "      <td>Most accept that he lived, taught and founded ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5438</th>\n",
       "      <td>Ethical</td>\n",
       "      <td>Ethical commitments in anthropology include no...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>French</td>\n",
       "      <td>Portuguese and their allied British troops fou...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>NORP</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   answer                                           sentence  \\\n",
       "4540           Republican  New York City has not been carried by a Republ...   \n",
       "5828  Galician-Portuguese  Portuguese is a Romance language that originat...   \n",
       "6391             monastic  Most accept that he lived, taught and founded ...   \n",
       "5438              Ethical  Ethical commitments in anthropology include no...   \n",
       "5493               French  Portuguese and their allied British troops fou...   \n",
       "\n",
       "      wordCount  isSingleToken   NER  POS TAG   DEP          shape  isAlpha  \\\n",
       "4540          1           True  NORP  ADJ  JJ  ROOT          Xxxxx     True   \n",
       "5828          1           True  NORP  ADJ  JJ  amod  Xxxxx - Xxxxx     True   \n",
       "6391          1           True        ADJ  JJ  ROOT           xxxx     True   \n",
       "5438          1           True        ADJ  JJ  ROOT          Xxxxx     True   \n",
       "5493          1           True  NORP  ADJ  JJ  ROOT          Xxxxx     True   \n",
       "\n",
       "      isStop  \n",
       "4540   False  \n",
       "5828   False  \n",
       "6391   False  \n",
       "5438   False  \n",
       "5493   False  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[(answersDf['POS'] == 'ADJ') & (answersDf['wordCount'] == 1)].sample(n=5, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>Flawless</td>\n",
       "      <td>She would later align herself more publicly wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8044</th>\n",
       "      <td>taboo</td>\n",
       "      <td>However, Western, South Asian, African, and Mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>destroyed</td>\n",
       "      <td>The definition upholds the centrality of inten...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7879</th>\n",
       "      <td>Neutering</td>\n",
       "      <td>Neutering reduces problems caused by hypersexu...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBG</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>helmet</td>\n",
       "      <td>However, as Hyrule Castle collapses, it is rev...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         answer                                           sentence  wordCount  \\\n",
       "649    Flawless  She would later align herself more publicly wi...          1   \n",
       "8044      taboo  However, Western, South Asian, African, and Mi...          1   \n",
       "1269  destroyed  The definition upholds the centrality of inten...          1   \n",
       "7879  Neutering  Neutering reduces problems caused by hypersexu...          1   \n",
       "2893     helmet  However, as Hyrule Castle collapses, it is rev...          1   \n",
       "\n",
       "      isSingleToken NER   POS  TAG   DEP  shape  isAlpha  isStop  \n",
       "649            True      VERB   VB  ROOT  Xxxxx     True   False  \n",
       "8044           True      VERB   VB  ROOT   xxxx     True   False  \n",
       "1269           True      VERB  VBN  ROOT   xxxx     True   False  \n",
       "7879           True      VERB  VBG  ROOT  Xxxxx     True   False  \n",
       "2893           True      VERB   VB  ROOT   xxxx     True   False  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[(answersDf['POS'] == 'VERB') & (answersDf['wordCount'] == 1)].sample(n=5, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the symbols are multi word answers, with some dollar signs infront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>sentence</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>isSingleToken</th>\n",
       "      <th>NER</th>\n",
       "      <th>POS</th>\n",
       "      <th>TAG</th>\n",
       "      <th>DEP</th>\n",
       "      <th>shape</th>\n",
       "      <th>isAlpha</th>\n",
       "      <th>isStop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>$26 million</td>\n",
       "      <td>The association has also collected a total of ...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>$ dd xxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9259</th>\n",
       "      <td>US$2.5 trillion</td>\n",
       "      <td>During the last quarter of 2008, these central...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>XX$ d.d xxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>$70 trillion</td>\n",
       "      <td>In a Peabody Award winning program, NPR corres...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>$ dd xxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>$70.4 million</td>\n",
       "      <td>The film ended up grossing $70.4 million in it...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>$ dd.d xxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>$772 million</td>\n",
       "      <td>Many donated through text messaging on mobile ...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>SYM</td>\n",
       "      <td>$</td>\n",
       "      <td>quantmod</td>\n",
       "      <td>$ ddd xxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               answer                                           sentence  \\\n",
       "3777      $26 million  The association has also collected a total of ...   \n",
       "9259  US$2.5 trillion  During the last quarter of 2008, these central...   \n",
       "9067     $70 trillion  In a Peabody Award winning program, NPR corres...   \n",
       "3252    $70.4 million  The film ended up grossing $70.4 million in it...   \n",
       "3675     $772 million  Many donated through text messaging on mobile ...   \n",
       "\n",
       "      wordCount  isSingleToken    NER  POS TAG       DEP         shape  \\\n",
       "3777          3           True  MONEY  SYM   $  quantmod     $ dd xxxx   \n",
       "9259          4           True  MONEY  SYM   $  quantmod  XX$ d.d xxxx   \n",
       "9067          3           True  MONEY  SYM   $  quantmod     $ dd xxxx   \n",
       "3252          3           True  MONEY  SYM   $  quantmod   $ dd.d xxxx   \n",
       "3675          3           True  MONEY  SYM   $  quantmod    $ ddd xxxx   \n",
       "\n",
       "      isAlpha  isStop  \n",
       "3777    False   False  \n",
       "9259    False   False  \n",
       "9067    False   False  \n",
       "3252    False   False  \n",
       "3675    False   False  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersDf[(answersDf['POS'] == 'SYM')].sample(n=5, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers in a bigger picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the highlighted answers.\n",
    "\n",
    "I suspect:\n",
    "\n",
    "1. There are other (many more?) obviously good words for answers that were just not selected.\n",
    "2. There are some sentences that just don't contain answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlightAnswers(titleId, paragraphId):\n",
    "\n",
    "    paragraph = df['data'][titleId]['paragraphs'][paragraphId]['context']\n",
    "    \n",
    "    answers = df['data'][titleId]['paragraphs'][paragraphId]['qas']\n",
    "\n",
    "    #Get answer starts and answer length\n",
    "    answerPosition = {}\n",
    "    for answer in answers:\n",
    "        answerStart = answer['answers'][0]['answer_start']\n",
    "        answerLength = len(answer['answers'][0]['text'])\n",
    "\n",
    "        answerPosition[answerStart] = answerLength\n",
    "\n",
    "    #Bold answers\n",
    "    shiftStart = 0\n",
    "    highlightedText = ''\n",
    "    currentPlaceInText = 0\n",
    "    \n",
    "    #Append text between previous answer and current answer + bold sign + answer + bold sign\n",
    "    for answerStart in sorted(answerPosition.keys()):\n",
    "        highlightedText += paragraph[currentPlaceInText:answerStart]\n",
    "        highlightedText += '**'\n",
    "        highlightedText += paragraph[answerStart:answerStart + answerPosition[answerStart]]\n",
    "        highlightedText += '**'\n",
    "        \n",
    "        currentPlaceInText = answerStart + answerPosition[answerStart]\n",
    "    \n",
    "    #Append the remaining text after the last answer\n",
    "    highlightedText += paragraph[currentPlaceInText:len(paragraph)]\n",
    "\n",
    "    #Diplay the highlighted text\n",
    "    display(Markdown(highlightedText))  #this format can make ** ** bold work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Located approximately 250 kilometres (**160** mi) east of Puerto Rico and the nearer Virgin Islands, St. Barthélemy lies immediately southeast of the islands of Saint Martin and Anguilla. It is one of **the Renaissance** Islands. St. Barthélemy is separated from Saint Martin by **the Saint-Barthélemy Channel**. It lies northeast of Saba and St Eustatius, and north of St Kitts. Some small **satellite islets** belong to St. Barthélemy including Île Chevreau (Île Bonhomme), Île Frégate, Île Toc Vers, Île Tortue and Gros Îlets (Îlots Syndare). A much bigger islet, Île Fourchue, lies on the north of the island, in the Saint-Barthélemy Channel. Other rocky islets which include Coco, the Roques (or **little Turtle rocks**), the Goat, and the Sugarloaf."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titleId = 24\n",
    "paragraphId = 0\n",
    "\n",
    "highlightAnswers(titleId, paragraphId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Inappropriate antibiotic treatment and overuse** of antibiotics have contributed to the emergence of antibiotic-resistant bacteria. **Self prescription** of antibiotics is an example of misuse. Many antibiotics are frequently prescribed to treat symptoms or diseases that do not respond to antibiotics or that are likely to resolve without treatment. Also, incorrect or suboptimal antibiotics are prescribed for certain bacterial infections. The **overuse of antibiotics**, like penicillin and erythromycin, has been associated with emerging antibiotic resistance since the 1950s. Widespread usage of antibiotics in hospitals has also been associated with increases in bacterial strains and species that no longer respond to treatment with the most common antibiotics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titleId = 4\n",
    "paragraphId = 12\n",
    "\n",
    "highlightAnswers(titleId, paragraphId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the **endurance running hypothesis**, long-distance running as in **persistence hunting**, a method still practiced by **some hunter-gatherer groups** in modern times, was likely the driving evolutionary force leading to the evolution of certain human characteristics. This hypothesis does not necessarily contradict the **scavenging hypothesis**: **both subsistence strategies** could have been in use – sequentially, alternating or even simultaneously."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titleId = 52\n",
    "paragraphId = 4\n",
    "\n",
    "highlightAnswers(titleId, paragraphId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The first commercially successful true engine, in that it could generate power and transmit it to a machine, was the **atmospheric engine**, invented by **Thomas Newcomen** around **1712**. It was an improvement over Savery's **steam pump**, using a piston as proposed by **Papin**. Newcomen's engine was relatively inefficient, and in most cases was used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths hitherto impossible, and also for providing a reusable water supply for driving waterwheels at factories sited away from a suitable \"head\". Water that had passed over the wheel was pumped back up into a storage reservoir above the wheel."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titleId = 453\n",
    "paragraphId = 1\n",
    "\n",
    "highlightAnswers(titleId, paragraphId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It definetely seems that there are a lot more words that could become good answers. But I'm optimistic I can extract the selected word's features even if I don't have all of the possible answer words.\n",
    "\n",
    "At first glance, it seems like the answers are spread troughout the entire text and there aren't as many sentences without an answers. Though a better experiment would be to just count the sentences without answers agaisnt the ones with. \n",
    "But I don't see a large enough benefit to do it (deadline aproaching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks\n",
    "Another neat thing spacy gives us is noun chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the school\n",
      "a Catholic character\n",
      "the Main Building's gold dome\n",
      "a golden statue\n",
      "the Virgin Mary\n",
      "front\n",
      "the Main Building\n",
      "it\n",
      "a copper statue\n",
      "Christ\n",
      "arms\n",
      "the legend\n",
      "\"Venite Ad Me Omnes\n",
      "the Main Building\n",
      "the Basilica\n",
      "the Sacred Heart\n",
      "the basilica\n",
      "the Grotto\n",
      "a Marian place\n",
      "prayer\n",
      "reflection\n",
      "It\n",
      "a replica\n",
      "the grotto\n",
      "Lourdes\n",
      "France\n",
      "the Virgin Mary\n",
      "the end\n",
      "the main drive\n",
      "a direct line\n",
      "that\n",
      "3 statues\n",
      "the Gold Dome\n",
      "a simple, modern stone statue\n",
      "Mary\n"
     ]
    }
   ],
   "source": [
    "text = df['data'][0]['paragraphs'][0]['context']\n",
    "doc = nlp(text)\n",
    "\n",
    "for noun_chunk in doc.noun_chunks:\n",
    "    print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is **a golden statue of the Virgin Mary**. Immediately in front of the Main Building and facing it, is **a copper statue of Christ** with arms upraised with the legend \"Venite Ad Me Omnes\". Next to **the Main Building** is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, **a Marian place of prayer and reflection**. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to **Saint Bernadette Soubirous** in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titleId = 0\n",
    "paragraphId = 0\n",
    "\n",
    "highlightAnswers(titleId, paragraphId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first paragraph we have 2 of the answers entirely recognized as noun chunks:\n",
    "1. **the Main Building**\n",
    "2. **Saint Bernadette Soubirous**\n",
    "\n",
    "While the other 3 answers are partially cut:\n",
    "1. **a golden statue** of the Virgin Mary\n",
    "2. **a copper statue** of *Christ* \n",
    "3. **a Marian place** of *prayer* and *reflection*\n",
    "\n",
    "Though I would argue that all of the other noun chunks would make great answers.\n",
    "I could potentially use only noun chunks for the answers and sacrifice the verbs and adjectives. But the noun chunks are mostly multi-word tokens. That would pose a problem with my features:\n",
    "1. **Part of speech** - Coulnd't really do it on multiple words.\n",
    "2. **TF-IDF** - Would need to modify it by either getting the aggreate of the single words or scoring the entire noun chunk... or both.\n",
    "3. **Title similarity** - Aggregation of the single words.\n",
    "4. **Incorrect answers** - That would be tricky, because I would need to find similar words for each word in the chunk and mix and match with the other similar words... That is bound to produce some inadequte mixes. But it still may not be a bad thing if I rely on a final filtering by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
